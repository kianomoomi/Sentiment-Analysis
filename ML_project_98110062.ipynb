{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6tjVIp1F3OAW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0z-DUaS56Fu",
        "outputId": "6200a757-1d20-491c-9fa5-7010437266b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data-train.csv')\n",
        "X =  df.drop('Sentiment', axis=1)\n",
        "y = df['Sentiment']\n",
        "X = X.values\n",
        "y = y.values"
      ],
      "metadata": {
        "id": "jHckyjEn4e_R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count frequency of each label\n",
        "from collections import Counter\n",
        "label_counts = Counter(y)\n",
        "label_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neFC4Ypy90tO",
        "outputId": "4bc616b8-aa0b-4553-df17-4bbbf61a2df4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 27084, 2: 79064, 3: 32714, 4: 9160, 0: 7026})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(sampling_strategy={1: label_counts[1] // 3, 2: label_counts[2] // 9, 3: label_counts[3] // 4})\n",
        "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "label_counts = Counter(y_resampled)\n",
        "label_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZWjkZx5-Ege",
        "outputId": "3db5e5a6-318a-449c-a92e-4d0f80e01654"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 7026, 1: 9028, 2: 8784, 3: 8178, 4: 9160})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i will only keep the sentence as the train dataset and remove other two columns\n",
        "X_resampled = X_resampled[:, 2]"
      ],
      "metadata": {
        "id": "xq8j251EIMkS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do preprocessing\n",
        "# make them lowercase\n",
        "X_resampled = [sentence.lower() for sentence in X_resampled]\n",
        "# remove punctuation\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "X_resampled = [tokenizer.tokenize(sentence) for sentence in X_resampled]\n",
        "# remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "X_resampled = [[word for word in sentence if word not in stop_words] for sentence in X_resampled]\n",
        "# do lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "X_resampled = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in X_resampled]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Rw_j47jFIvw",
        "outputId": "0d7aa2a7-5974-48d8-b0b7-ec2305944866"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=41)"
      ],
      "metadata": {
        "id": "-Vn1kzyR5wtP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the preprocessed sentences into numerical features using TF-IDF vectorization\n",
        "# this is the approcach 1 mentioned in the doc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features = 600)\n",
        "X_train_tfidf = vectorizer.fit_transform([' '.join(sentence) for sentence in X_train])\n",
        "X_test_tfidf = vectorizer.transform([' '.join(sentence) for sentence in X_test])\n",
        "X_validation_tfidf = vectorizer.transform([' '.join(sentence) for sentence in X_val])"
      ],
      "metadata": {
        "id": "Cd796q0MJk-5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now i want to use fasttext to do feature extraction and get embeddings for a sentence\n",
        "# this the approach 2 mentioned in the doc\n",
        "import fasttext\n",
        "with open('train.txt', 'w') as f:\n",
        "    for sentence in X_train:\n",
        "        f.write(' '.join(sentence) + '\\n')\n",
        "\n",
        "params = {\n",
        "    'lr': 0.1,\n",
        "    'epoch': 15,\n",
        "    'dim': 500,\n",
        "    'ws': 5,\n",
        "    'minCount': 5,\n",
        "    'neg': 5\n",
        "}\n",
        "\n",
        "# Train the FastText model\n",
        "model = fasttext.train_unsupervised('train.txt', **params)\n",
        "\n",
        "# Use the trained FastText model to get sentence embeddings\n",
        "X_train_fasttext = [model.get_sentence_vector(' '.join(sentence)) for sentence in X_train]\n",
        "X_test_fasttext = [model.get_sentence_vector(' '.join(sentence)) for sentence in X_test]\n",
        "X_validation_fasttext = [model.get_sentence_vector(' '.join(sentence)) for sentence in X_val]\n"
      ],
      "metadata": {
        "id": "rUhTDXXcKbsW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_val_encoded = to_categorical(y_val)\n",
        "y_test_encoded = to_categorical(y_test)\n",
        "X_train_tfidf = X_train_tfidf.toarray()\n",
        "X_train_fasttext = np.array(X_train_fasttext)\n",
        "X_validation_tfidf = X_validation_tfidf.toarray()\n",
        "X_validation_fasttext = np.array(X_validation_fasttext)\n",
        "X_test_tfidf = X_test_tfidf.toarray()\n",
        "X_test_fasttext = np.array(X_test_fasttext)\n",
        "\n",
        "\n",
        "# Define the neural network model for tfidf\n",
        "neural_net_tfidf = Sequential()\n",
        "neural_net_tfidf.add(Dense(256, activation='relu', input_shape=(X_train_tfidf.shape[1],)))\n",
        "neural_net_tfidf.add(BatchNormalization())\n",
        "neural_net_tfidf.add(Dropout(0.5))\n",
        "neural_net_tfidf.add(Dense(128, activation='relu'))\n",
        "neural_net_tfidf.add(BatchNormalization())\n",
        "neural_net_tfidf.add(Dropout(0.5))\n",
        "neural_net_tfidf.add(Dense(64, activation='relu'))\n",
        "neural_net_tfidf.add(BatchNormalization())\n",
        "neural_net_tfidf.add(Dropout(0.5))\n",
        "neural_net_tfidf.add(Dense(5, activation='softmax'))\n",
        "neural_net_tfidf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "neural_net_tfidf.fit(X_train_tfidf, y_train_encoded, validation_data=(X_validation_tfidf, y_val_encoded), epochs=40)\n"
      ],
      "metadata": {
        "id": "rSSQATDrdZ6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53b9392-5d7b-4f1a-8ae2-4e0be8abbcb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "844/844 [==============================] - 7s 6ms/step - loss: 1.7719 - accuracy: 0.2818 - val_loss: 1.4040 - val_accuracy: 0.4069\n",
            "Epoch 2/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.4216 - accuracy: 0.3853 - val_loss: 1.3477 - val_accuracy: 0.4238\n",
            "Epoch 3/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.3632 - accuracy: 0.4155 - val_loss: 1.3344 - val_accuracy: 0.4327\n",
            "Epoch 4/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.3361 - accuracy: 0.4279 - val_loss: 1.3170 - val_accuracy: 0.4465\n",
            "Epoch 5/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.3088 - accuracy: 0.4478 - val_loss: 1.3116 - val_accuracy: 0.4459\n",
            "Epoch 6/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.2877 - accuracy: 0.4550 - val_loss: 1.3007 - val_accuracy: 0.4489\n",
            "Epoch 7/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2700 - accuracy: 0.4648 - val_loss: 1.2845 - val_accuracy: 0.4566\n",
            "Epoch 8/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.2416 - accuracy: 0.4807 - val_loss: 1.2752 - val_accuracy: 0.4573\n",
            "Epoch 9/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.2255 - accuracy: 0.4865 - val_loss: 1.2679 - val_accuracy: 0.4665\n",
            "Epoch 10/40\n",
            "844/844 [==============================] - 5s 7ms/step - loss: 1.2066 - accuracy: 0.4962 - val_loss: 1.2631 - val_accuracy: 0.4693\n",
            "Epoch 11/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1910 - accuracy: 0.5031 - val_loss: 1.2617 - val_accuracy: 0.4635\n",
            "Epoch 12/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1769 - accuracy: 0.5124 - val_loss: 1.2619 - val_accuracy: 0.4585\n",
            "Epoch 13/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1668 - accuracy: 0.5183 - val_loss: 1.2610 - val_accuracy: 0.4671\n",
            "Epoch 14/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1505 - accuracy: 0.5249 - val_loss: 1.2636 - val_accuracy: 0.4678\n",
            "Epoch 15/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1379 - accuracy: 0.5335 - val_loss: 1.2679 - val_accuracy: 0.4635\n",
            "Epoch 16/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1272 - accuracy: 0.5363 - val_loss: 1.2744 - val_accuracy: 0.4727\n",
            "Epoch 17/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1183 - accuracy: 0.5425 - val_loss: 1.2704 - val_accuracy: 0.4684\n",
            "Epoch 18/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1080 - accuracy: 0.5462 - val_loss: 1.2787 - val_accuracy: 0.4653\n",
            "Epoch 19/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1028 - accuracy: 0.5500 - val_loss: 1.2877 - val_accuracy: 0.4631\n",
            "Epoch 20/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0897 - accuracy: 0.5565 - val_loss: 1.2842 - val_accuracy: 0.4627\n",
            "Epoch 21/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0782 - accuracy: 0.5611 - val_loss: 1.2937 - val_accuracy: 0.4664\n",
            "Epoch 22/40\n",
            "844/844 [==============================] - 5s 7ms/step - loss: 1.0736 - accuracy: 0.5612 - val_loss: 1.3017 - val_accuracy: 0.4658\n",
            "Epoch 23/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0747 - accuracy: 0.5639 - val_loss: 1.2896 - val_accuracy: 0.4635\n",
            "Epoch 24/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0656 - accuracy: 0.5692 - val_loss: 1.2912 - val_accuracy: 0.4652\n",
            "Epoch 25/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.0569 - accuracy: 0.5754 - val_loss: 1.3019 - val_accuracy: 0.4677\n",
            "Epoch 26/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0513 - accuracy: 0.5722 - val_loss: 1.3074 - val_accuracy: 0.4721\n",
            "Epoch 27/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0453 - accuracy: 0.5792 - val_loss: 1.3144 - val_accuracy: 0.4680\n",
            "Epoch 28/40\n",
            "844/844 [==============================] - 11s 13ms/step - loss: 1.0434 - accuracy: 0.5766 - val_loss: 1.3130 - val_accuracy: 0.4604\n",
            "Epoch 29/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0365 - accuracy: 0.5857 - val_loss: 1.3141 - val_accuracy: 0.4638\n",
            "Epoch 30/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.0300 - accuracy: 0.5884 - val_loss: 1.3208 - val_accuracy: 0.4671\n",
            "Epoch 31/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0244 - accuracy: 0.5877 - val_loss: 1.3254 - val_accuracy: 0.4661\n",
            "Epoch 32/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0216 - accuracy: 0.5894 - val_loss: 1.3252 - val_accuracy: 0.4637\n",
            "Epoch 33/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.0231 - accuracy: 0.5863 - val_loss: 1.3307 - val_accuracy: 0.4590\n",
            "Epoch 34/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0126 - accuracy: 0.5907 - val_loss: 1.3345 - val_accuracy: 0.4665\n",
            "Epoch 35/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0136 - accuracy: 0.5936 - val_loss: 1.3261 - val_accuracy: 0.4649\n",
            "Epoch 36/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.0128 - accuracy: 0.5943 - val_loss: 1.3301 - val_accuracy: 0.4601\n",
            "Epoch 37/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.0029 - accuracy: 0.5956 - val_loss: 1.3515 - val_accuracy: 0.4616\n",
            "Epoch 38/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 0.9955 - accuracy: 0.5977 - val_loss: 1.3565 - val_accuracy: 0.4628\n",
            "Epoch 39/40\n",
            "844/844 [==============================] - 7s 8ms/step - loss: 1.0024 - accuracy: 0.5974 - val_loss: 1.3396 - val_accuracy: 0.4656\n",
            "Epoch 40/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 0.9883 - accuracy: 0.6022 - val_loss: 1.3504 - val_accuracy: 0.4655\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93208ad480>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model for fasttext\n",
        "neural_net_fasttext = Sequential()\n",
        "neural_net_fasttext.add(Dense(256, activation='relu', input_shape=(X_train_fasttext.shape[1],)))\n",
        "neural_net_fasttext.add(BatchNormalization())\n",
        "neural_net_fasttext.add(Dropout(0.5))\n",
        "\n",
        "neural_net_fasttext.add(Dense(128, activation='relu'))\n",
        "neural_net_fasttext.add(BatchNormalization())\n",
        "neural_net_fasttext.add(Dropout(0.5))\n",
        "\n",
        "neural_net_fasttext.add(Dense(64, activation='relu'))\n",
        "neural_net_fasttext.add(BatchNormalization())\n",
        "neural_net_fasttext.add(Dropout(0.5))\n",
        "\n",
        "neural_net_fasttext.add(Dense(32, activation='relu'))\n",
        "neural_net_fasttext.add(BatchNormalization())\n",
        "neural_net_fasttext.add(Dropout(0.5))\n",
        "\n",
        "neural_net_fasttext.add(Dense(16, activation='relu'))\n",
        "neural_net_fasttext.add(BatchNormalization())\n",
        "neural_net_fasttext.add(Dropout(0.5))\n",
        "\n",
        "neural_net_fasttext.add(Dense(5, activation='softmax'))\n",
        "\n",
        "neural_net_fasttext.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "neural_net_fasttext.fit(X_train_fasttext, y_train_encoded, validation_data=(X_validation_fasttext, y_val_encoded), epochs=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVOv0sD0q7tJ",
        "outputId": "8106dbf7-0320-4add-b81c-84c866a7c94f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "844/844 [==============================] - 7s 6ms/step - loss: 1.8523 - accuracy: 0.2388 - val_loss: 1.5120 - val_accuracy: 0.3317\n",
            "Epoch 2/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.5096 - accuracy: 0.3108 - val_loss: 1.4236 - val_accuracy: 0.3678\n",
            "Epoch 3/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.4468 - accuracy: 0.3492 - val_loss: 1.3735 - val_accuracy: 0.3877\n",
            "Epoch 4/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.4133 - accuracy: 0.3699 - val_loss: 1.3472 - val_accuracy: 0.4052\n",
            "Epoch 5/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.3933 - accuracy: 0.3883 - val_loss: 1.3192 - val_accuracy: 0.4249\n",
            "Epoch 6/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.3757 - accuracy: 0.4003 - val_loss: 1.3077 - val_accuracy: 0.4265\n",
            "Epoch 7/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.3612 - accuracy: 0.4098 - val_loss: 1.2965 - val_accuracy: 0.4354\n",
            "Epoch 8/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.3471 - accuracy: 0.4174 - val_loss: 1.2827 - val_accuracy: 0.4395\n",
            "Epoch 9/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.3310 - accuracy: 0.4258 - val_loss: 1.2849 - val_accuracy: 0.4465\n",
            "Epoch 10/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.3222 - accuracy: 0.4355 - val_loss: 1.2789 - val_accuracy: 0.4388\n",
            "Epoch 11/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.3135 - accuracy: 0.4397 - val_loss: 1.2569 - val_accuracy: 0.4554\n",
            "Epoch 12/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.2962 - accuracy: 0.4438 - val_loss: 1.2507 - val_accuracy: 0.4567\n",
            "Epoch 13/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.2969 - accuracy: 0.4457 - val_loss: 1.2521 - val_accuracy: 0.4490\n",
            "Epoch 14/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2827 - accuracy: 0.4549 - val_loss: 1.2420 - val_accuracy: 0.4628\n",
            "Epoch 15/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2730 - accuracy: 0.4567 - val_loss: 1.2343 - val_accuracy: 0.4601\n",
            "Epoch 16/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.2653 - accuracy: 0.4622 - val_loss: 1.2327 - val_accuracy: 0.4579\n",
            "Epoch 17/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2590 - accuracy: 0.4622 - val_loss: 1.2182 - val_accuracy: 0.4711\n",
            "Epoch 18/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2513 - accuracy: 0.4663 - val_loss: 1.2195 - val_accuracy: 0.4755\n",
            "Epoch 19/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2415 - accuracy: 0.4716 - val_loss: 1.2137 - val_accuracy: 0.4732\n",
            "Epoch 20/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2359 - accuracy: 0.4779 - val_loss: 1.2078 - val_accuracy: 0.4742\n",
            "Epoch 21/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.2316 - accuracy: 0.4750 - val_loss: 1.2058 - val_accuracy: 0.4764\n",
            "Epoch 22/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.2236 - accuracy: 0.4787 - val_loss: 1.1934 - val_accuracy: 0.4865\n",
            "Epoch 23/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2155 - accuracy: 0.4795 - val_loss: 1.1950 - val_accuracy: 0.4801\n",
            "Epoch 24/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2119 - accuracy: 0.4869 - val_loss: 1.1894 - val_accuracy: 0.4893\n",
            "Epoch 25/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.2084 - accuracy: 0.4869 - val_loss: 1.1904 - val_accuracy: 0.4830\n",
            "Epoch 26/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.2063 - accuracy: 0.4901 - val_loss: 1.1859 - val_accuracy: 0.4876\n",
            "Epoch 27/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.2040 - accuracy: 0.4887 - val_loss: 1.1786 - val_accuracy: 0.4956\n",
            "Epoch 28/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.1871 - accuracy: 0.4930 - val_loss: 1.1797 - val_accuracy: 0.4874\n",
            "Epoch 29/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1849 - accuracy: 0.4936 - val_loss: 1.1716 - val_accuracy: 0.4916\n",
            "Epoch 30/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1856 - accuracy: 0.4982 - val_loss: 1.1737 - val_accuracy: 0.4881\n",
            "Epoch 31/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.1774 - accuracy: 0.5022 - val_loss: 1.1729 - val_accuracy: 0.4975\n",
            "Epoch 32/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1784 - accuracy: 0.5004 - val_loss: 1.1709 - val_accuracy: 0.4945\n",
            "Epoch 33/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1706 - accuracy: 0.5030 - val_loss: 1.1726 - val_accuracy: 0.4917\n",
            "Epoch 34/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.1726 - accuracy: 0.5032 - val_loss: 1.1667 - val_accuracy: 0.5010\n",
            "Epoch 35/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.1636 - accuracy: 0.5075 - val_loss: 1.1661 - val_accuracy: 0.4917\n",
            "Epoch 36/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1578 - accuracy: 0.5094 - val_loss: 1.1614 - val_accuracy: 0.5009\n",
            "Epoch 37/40\n",
            "844/844 [==============================] - 4s 5ms/step - loss: 1.1594 - accuracy: 0.5075 - val_loss: 1.1629 - val_accuracy: 0.4961\n",
            "Epoch 38/40\n",
            "844/844 [==============================] - 5s 6ms/step - loss: 1.1560 - accuracy: 0.5097 - val_loss: 1.1584 - val_accuracy: 0.4969\n",
            "Epoch 39/40\n",
            "844/844 [==============================] - 5s 5ms/step - loss: 1.1580 - accuracy: 0.5102 - val_loss: 1.1587 - val_accuracy: 0.4964\n",
            "Epoch 40/40\n",
            "844/844 [==============================] - 6s 7ms/step - loss: 1.1475 - accuracy: 0.5150 - val_loss: 1.1563 - val_accuracy: 0.4994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91bd62e470>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_tfidf = neural_net_tfidf.evaluate(X_test_tfidf, y_test_encoded)[1]\n",
        "print(f'accuracy of tfidf: {accuracy_tfidf}')\n",
        "accuracy_fasttext = neural_net_fasttext.evaluate(X_test_fasttext, y_test_encoded)[1]\n",
        "print(f'accuracy of fasttext: {accuracy_fasttext}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3lm0c6xrrjT",
        "outputId": "280957fa-260e-4181-90bf-1a711d66c184"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264/264 [==============================] - 0s 2ms/step - loss: 1.3685 - accuracy: 0.4539\n",
            "accuracy of tfidf: 0.45388808846473694\n",
            "264/264 [==============================] - 0s 2ms/step - loss: 1.1536 - accuracy: 0.5013\n",
            "accuracy of fasttext: 0.5013039112091064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate precision\n",
        "from sklearn.metrics import precision_score\n",
        "precision_tfidf = precision_score(y_test_encoded.argmax(axis=1), neural_net_tfidf.predict(X_test_tfidf).argmax(axis=1), average='macro')\n",
        "precision_fasttext = precision_score(y_test_encoded.argmax(axis=1), neural_net_fasttext.predict(X_test_fasttext).argmax(axis=1), average='macro')\n",
        "print(f'precision of tfidf: {precision_tfidf}')\n",
        "print(f'precision of fasttext: {precision_fasttext}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D1ksdyQQgyE",
        "outputId": "4f23b769-7ac1-4f0d-d2d2-719bc096c1ef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264/264 [==============================] - 2s 5ms/step\n",
            "264/264 [==============================] - 2s 2ms/step\n",
            "precision of tfidf: 0.47579289187750906\n",
            "precision of fasttext: 0.47989077035612926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate recall\n",
        "from sklearn.metrics import recall_score\n",
        "recall_tfidf = recall_score(y_test_encoded.argmax(axis=1), neural_net_tfidf.predict(X_test_tfidf).argmax(axis=1), average='macro')\n",
        "recall_fasttext = recall_score(y_test_encoded.argmax(axis=1), neural_net_fasttext.predict(X_test_fasttext).argmax(axis=1), average='macro')\n",
        "print(f'recall of tfidf: {recall_tfidf}')\n",
        "print(f'recall of fasttext: {recall_fasttext}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozmVztVFTETC",
        "outputId": "69a3dda5-a504-48cf-e5a7-86fc97ca0c17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264/264 [==============================] - 0s 1ms/step\n",
            "264/264 [==============================] - 0s 1ms/step\n",
            "recall of tfidf: 0.4508729970051754\n",
            "recall of fasttext: 0.5018820367573813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate f1-score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_tfidf = f1_score(y_test_encoded.argmax(axis=1), neural_net_tfidf.predict(X_test_tfidf).argmax(axis=1), average='macro')\n",
        "f1_fasttext = f1_score(y_test_encoded.argmax(axis=1), neural_net_fasttext.predict(X_test_fasttext).argmax(axis=1), average='macro')\n",
        "print(f'f1-score of tfidf: {f1_tfidf}')\n",
        "print(f'f1-score of fasttext: {f1_fasttext}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F05K1chihcTw",
        "outputId": "54a799b4-8755-4eec-af2b-c61ff9345e39"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264/264 [==============================] - 1s 2ms/step\n",
            "264/264 [==============================] - 1s 2ms/step\n",
            "f1-score of tfidf: 0.44403380655326086\n",
            "f1-score of fasttext: 0.47823633986779424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normalized confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_tfidf = confusion_matrix(y_test_encoded.argmax(axis=1), neural_net_tfidf.predict(X_test_tfidf).argmax(axis=1))\n",
        "confusion_fasttext = confusion_matrix(y_test_encoded.argmax(axis=1), neural_net_fasttext.predict(X_test_fasttext).argmax(axis=1))\n",
        "confusion_tfidf_normalized = confusion_tfidf / confusion_tfidf.sum(axis=1)[:, np.newaxis]\n",
        "confusion_fasttext_normalized = confusion_fasttext / confusion_fasttext.sum(axis=1)[:, np.newaxis]\n",
        "print(f'normalized confusion matrix of tfidf:\\n {confusion_tfidf_normalized}')\n",
        "print(f'\\nnormalized confusion matrix of fasttext:\\n {confusion_fasttext_normalized}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRm3qu0OiCDp",
        "outputId": "eadaf66f-97ec-4289-a557-1ec0334d49fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264/264 [==============================] - 0s 1ms/step\n",
            "264/264 [==============================] - 0s 2ms/step\n",
            "normalized confusion matrix of tfidf:\n",
            " [[0.43784153 0.2260929  0.26775956 0.03551913 0.03278689]\n",
            " [0.1615938  0.31045932 0.42280022 0.0647482  0.04039845]\n",
            " [0.03195816 0.131319   0.71179547 0.08192911 0.04299826]\n",
            " [0.02216749 0.0862069  0.41133005 0.22229064 0.25800493]\n",
            " [0.01373626 0.03956044 0.18516484 0.18956044 0.57197802]]\n",
            "\n",
            "normalized confusion matrix of fasttext:\n",
            " [[0.69535519 0.20969945 0.03620219 0.01912568 0.03961749]\n",
            " [0.39679026 0.33204206 0.15495296 0.05810736 0.05810736]\n",
            " [0.08367228 0.21499128 0.49099361 0.12144102 0.0889018 ]\n",
            " [0.05972906 0.13485222 0.17426108 0.19211823 0.43903941]\n",
            " [0.01868132 0.05274725 0.02637363 0.1032967  0.7989011 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cells below this cell are related to kaggle competition"
      ],
      "metadata": {
        "id": "d-5-_-8uuC9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv('/content/pr-test-data.csv')\n",
        "X_test_df = test_df['Phrase'].values\n",
        "ID_column = test_df['ID'].values"
      ],
      "metadata": {
        "id": "Co3IBRMDuCck"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do preprocessing\n",
        "# make them lowercase\n",
        "X_test_df = [sentence.lower() for sentence in X_test_df]\n",
        "# remove punctuation\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "X_test_df = [tokenizer.tokenize(sentence) for sentence in X_test_df]\n",
        "# remove stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "X_test_df = [[word for word in sentence if word not in stop_words] for sentence in X_test_df]\n",
        "# do lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "X_test_df = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in X_test_df]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn8yE57XvAmH",
        "outputId": "275f77f4-de7e-4fb8-f09a-73deacffedea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_df_tfidf = vectorizer.transform([' '.join(sentence) for sentence in X_test_df])\n",
        "X_test_df_tfidf = X_test_df_tfidf.toarray()\n",
        "y_pred_tfidf = neural_net_tfidf.predict(X_test_df_tfidf).argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX-NJ_Mk-xep",
        "outputId": "893d7f56-d937-4327-b141-f05ec6a3f754"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_tfidf = pd.DataFrame({'Sentiment': y_pred_tfidf, 'ID':ID_column})\n",
        "submission_df_tfidf.to_csv('submission_tfidf.csv', index=False)"
      ],
      "metadata": {
        "id": "BIoKG6r-_N9a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_df_fasttext = [model.get_sentence_vector(' '.join(sentence)) for sentence in X_test_df]\n",
        "X_test_df_fasttext = np.array(X_test_df_fasttext)\n",
        "y_pred_fasttext = neural_net_fasttext.predict(X_test_df_fasttext).argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm9aT7uvvoAo",
        "outputId": "397908e3-ae18-4da3-943d-dfa45c78fd58"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_fasttext = pd.DataFrame({'Sentiment': y_pred_fasttext, 'ID':ID_column})\n",
        "submission_df_fasttext.to_csv('submission_fasttext.csv', index=False)"
      ],
      "metadata": {
        "id": "9C88P32l5YP7"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}